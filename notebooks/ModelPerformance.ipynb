{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Performance\n",
    "\n",
    "Get ready to put on your data science and ML engineering hats! In the next few lessons, you are going to learn how to evaluate the performance of ML models. Recall that after you train an ML model on past data, you can use that model to make predictions on new or previously unseen data. But how do you know if that model is useful?  In ML, when you hear the phrase \"model performance\", I want you to think about evaluating the quality of model predictions, commonly referred to as its **forecast skill** or **prediction skill**. This first lesson will focus on evaluating predictive modeling in the context of supervised learning, including both classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification Accuracy and Error Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Classification is about predicting a label, typically a discrete value. For example, an image of an animal may be classified as being a picture of a \"cat\" or \"dog\". There are many ways to measure the prediction skill of a classification model, but **accuracy** and **error rate** are the de facto standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accuracy\n",
    "Accuracy is the ratio of the correct predictions to the total number of predictions made.\n",
    "* Accuracy = Correct Predictions / Total Predictions\n",
    "\n",
    "90% and above for the accuracy of a predictive model is considered to be good, and it is common practice to aim for that level. \n",
    "\n",
    "### Error Rate\n",
    "You can also summarize model performance in terms of the error rate.\n",
    "* Error Rate = Incorrect Predictions / Total Predictions\n",
    "\n",
    "Accuracy and error rates are complements of each other and therefore you can calculate one from the other as follows:\n",
    "* Accuracy = 1 - Error Rate\n",
    "* Error Rate = 1 - Accuracy\n",
    "\n",
    "Consider a classifier that labels pictures as either cats or dogs and that, when tested on 12 pictures (8 cats and 4 dogs), produces the following results:\n",
    "* 9 Correct Predictions   = (9/12) = 0.75 \n",
    "* 3 Incorrect Predictions = (3/12) = 0.25\n",
    "\n",
    "Knowing that the classifier has an accuracy of 0.75 or 75%, does not provide any insight into where the classifier is not performing well. \n",
    "\n",
    "Is it more mistaking cats for dogs, or dogs for cats? or is it about the same? \n",
    "\n",
    "This is where a **confusion matrix** may prove useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Confusion Matrix\n",
    "A confusion matrix allows you to easily visualize classification performance and see where the model may be confusing two or more classes. \n",
    "\n",
    "<img src=\"img/confusion_matrix.png\" width=\"200\">\n",
    "\n",
    "In this confusion matrix, of the 8 cat pictures, the model predicted that 2 were dogs, and of the 4 dog pictures, it predicted that 1 was a cat. All correct predictions are located in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they are represented by values outside the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hands-On Classification Metrics: First Look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Scikit-learn is a free ML library for the Python programming language. It has 3 different programming interfaces for evaluating the quality of a modelâ€™s predictions:\n",
    "\n",
    "* Estimator Score Method\n",
    "* Scoring Parameter\n",
    "* Metrics Functions\n",
    "\n",
    "In this hands-on practice, you'll get experience using the scikit-learn metrics functions to measure the prediction skill of a binary classifier that distinguishes cats and dogs. \n",
    "\n",
    "First start by importing the scikit-learn metrics module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Assume that the actual and predicted values from the example are defined as follows, where cats belong to the class 0 and dogs belong to the class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_values = [0,0,0,0,0,0,0,0,1,1,1,1]\n",
    "predictions =   [1,1,0,0,0,0,0,0,1,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you can use the metrics functions to calculate the accuracy, error rate, and print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.0 % \n",
      "Error Rate: 25.0 % \n",
      "Confusion Matrix:\n",
      "[[6 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {metrics.accuracy_score(actual_values, predictions) * 100} % ')\n",
    "\n",
    "print(f'Error Rate: {(1 - metrics.accuracy_score(actual_values, predictions)) * 100} % ')\n",
    "\n",
    "print(f'Confusion Matrix:')\n",
    "\n",
    "print(metrics.confusion_matrix(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Precision, Recall, and F-Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a performance measure, classification accuracy has its limitations. One example where accuracy may be an inadequate performance measure is in the presence of class imbalance. For example, imagine a situation where a dataset of cat and dog images contains a large number of cat examples (majority class) and a small number of dog examples (minority class). On such a dataset, even unskillful models model may achieve high accuracy if the large number of examples from the majority class overwhelms those in the minority class.  \n",
    "\n",
    "An alternative to using classification accuracy is to use precision and recall metrics. \n",
    "\n",
    "However, prior to getting into precision and recall, it is important to dive deeper into the confusion matrix as it provides insight into both the performance of the model and the types of errors being made.\n",
    "\n",
    "### Confusion Matrix: Reloaded\n",
    "\n",
    "The results summary displayed in the confusion matrix consists of true predictions and false predictions.\n",
    "\n",
    "<img src=\"img/confusion_matrix_reloaded.png\" width=\"200\">\n",
    "\n",
    "True Predictions: \n",
    "  * TP: True Positives. \n",
    "    - Model predicted Yes, and actual value is Yes.\n",
    "  * TN: True Negatives. \n",
    "    - Model predicted No, and actual value is no.\n",
    "    \n",
    "False Predictions: \n",
    "  * FP: False Positives. \n",
    "    - Model predicted Yes, but actual value is No.\n",
    "  * FN: False Negatives. \n",
    "    - Model predicted No, but actual value is Yes.\n",
    "\n",
    "\n",
    "The **precision** and **recall** metrics are defined using the four terms TP, TN, FP, and FN in the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision quantifies the number of correct positive predictions made. It answers the question: When the model predicts yes, how often is it right? It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted.\n",
    "\n",
    "* Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "### Recall\n",
    "Recall quantifies the number of correct positive predictions made out of all of the positive predictions that could have been made. It answers the question: What percentage of the actual positives were identified? Therefore, unlike precision, recall provides an indication of missed positive predictions. It is calculated as the number of true positives divided by the total number of true positives and false negatives.\n",
    "\n",
    "* Recall = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "A predictive model with high recall and low precision returns many results, but most of its predicted labels are incorrect. On the other hand, a predictive model with high precision and low recall returns very few results, but most of its predicted labels are correct. The ideal predictive model has high precision and high recall, returning many results with most of its results labeled correctly.\n",
    "\n",
    "### F-Measure\n",
    "\n",
    "Precision and recall can be used to compute the **F-Measure** &mdash; a single metric that captures both properties. The traditional F measure is calculated as the harmonic mean of the two fractions.\n",
    "\n",
    "* F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "It is sometimes called the **F-Score** or **F1-Score** and is perhaps the most commonly used metric for imbalanced classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hands-On Classification Metrics: Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following code example demonstrates how precision, recall and the f1 score can be computed individually using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score is: 0.6\n",
      "Recall Score is: 0.75\n",
      "F1 Score: 0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "from sklearn import metrics\n",
    "\n",
    "#model prediction results\n",
    "actual_values = [0,0,0,0,0,0,0,0,1,1,1,1]\n",
    "predictions =   [1,1,0,0,0,0,0,0,1,1,1,0]\n",
    "\n",
    "#precision\n",
    "print(f'Precision Score is: {metrics.precision_score(actual_values, predictions)}')\n",
    "\n",
    "#recall\n",
    "print(f'Recall Score is: {metrics.recall_score(actual_values, predictions)}')\n",
    "\n",
    "#f1 score\n",
    "print('F1 Score:', metrics.f1_score(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can view a full classification report which includes the accuracy, precision, recall and f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80         8\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.73      0.75      0.73        12\n",
      "weighted avg       0.77      0.75      0.76        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(metrics.classification_report(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
